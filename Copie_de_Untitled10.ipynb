{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPp3cbj8p32Xzh9e2biQ3tI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HalimaLotfe/redbloodcells_disease_classification/blob/master/Copie_de_Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vMw1GbSD9XcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1a8ce8a-e861-4ae5-a7ee-d298c829ff6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount (\"/content/gdrive\")\n",
        "\n",
        "%cd /content/gdrive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from math import exp\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def random_downsampling(l, k):\n",
        "    '''\n",
        "    Take a random subset of k elements in a list of length l\n",
        "    '''\n",
        "    if l < k:\n",
        "        return np.asarray(list(range(l)))\n",
        "\n",
        "    return np.asarray(sorted(random.sample(list(range(l)), k=k)))\n",
        "\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor(\n",
        "        [\n",
        "            exp(-((x - window_size // 2) ** 2) / float(2 * sigma ** 2))\n",
        "            for x in range(window_size)\n",
        "        ]\n",
        "    )\n",
        "    return gauss / gauss.sum()\n",
        "\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    '''\n",
        "    Utils to create a window for SSIM\n",
        "    '''\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = Variable(\n",
        "        _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
        "    )\n",
        "    return window\n",
        "\n",
        "\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average=True):\n",
        "    '''\n",
        "    Utils to compute SSIM\n",
        "    '''\n",
        "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = (\n",
        "        F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
        "    )\n",
        "    sigma2_sq = (\n",
        "        F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
        "    )\n",
        "    sigma12 = (\n",
        "        F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel)\n",
        "        - mu1_mu2\n",
        "    )\n",
        "\n",
        "    C1 = 0.01 ** 2\n",
        "    C2 = 0.03 ** 2\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / (\n",
        "        (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)\n",
        "    )\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "\n",
        "def ssim(img1, img2, window_size=11, size_average=True):\n",
        "    '''\n",
        "    Computes the SSIM index between two images.\n",
        "    '''\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel)\n",
        "\n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "\n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
        "\n",
        "\n",
        "def getmax_sim(sim):\n",
        "    \"\"\"\n",
        "    Get the pair of images with the highest similarity. It is used to remove the most similar consecutive images\n",
        "    \"\"\"\n",
        "    m = 0\n",
        "    idx = None\n",
        "\n",
        "    for k, x in sim.items():\n",
        "        if m < x:\n",
        "            m = x\n",
        "            idx = k\n",
        "\n",
        "    return k\n",
        "\n",
        "\n",
        "# Downsampling function\n",
        "def ssim_downsampling(sequence, k):\n",
        "    \"\"\"\n",
        "    Perform the downsampling using pytorch (and the GPU, it is really really faster !)\n",
        "    It computes the ssim between each image and the next one. It then recursively select the most similar pair of images\n",
        "    and remove one of them. It stops when the number of images is equal to k\n",
        "    \"\"\"\n",
        "\n",
        "    sequence = sequence.to(\"cuda\")\n",
        "    l = sequence.shape[0]\n",
        "    keptindices = {i for i in range(l)}\n",
        "\n",
        "    if l < k:\n",
        "        return list(keptindices)\n",
        "\n",
        "    s1 = sequence[:-1]\n",
        "    s2 = sequence[1:]\n",
        "\n",
        "    similarities = ssim(s1, s2, size_average=False).squeeze().cpu().detach().numpy()\n",
        "\n",
        "    similarities = {(i, i + 1): similarities[i] for i in range(0, l - 1)}\n",
        "\n",
        "    while len(keptindices) > k:\n",
        "        key = getmax_sim(similarities)\n",
        "        similarities.pop(key)\n",
        "\n",
        "        nsim = float(\n",
        "            ssim(\n",
        "                sequence[key[0] - 1].unsqueeze(dim=0),\n",
        "                sequence[key[1]].unsqueeze(dim=0),\n",
        "                size_average=False,\n",
        "            )\n",
        "            .squeeze()\n",
        "            .cpu()\n",
        "            .detach()\n",
        "            .numpy()\n",
        "        )\n",
        "        similarities[(key[0] - 1, key[1])] = nsim\n",
        "\n",
        "        keptindices.remove(key[0])\n",
        "\n",
        "    return np.asarray(list(keptindices))\n",
        "\n",
        "\n",
        "def fmax_length(x):\n",
        "    l = 0\n",
        "\n",
        "    for s in x:\n",
        "        ltmp = len(s)\n",
        "        if ltmp > l:\n",
        "            l = ltmp\n",
        "    return l\n",
        "\n",
        "\n",
        "def pad_collate(batch, l=None):\n",
        "    \"\"\"\n",
        "    Pad a sequence of image with black images\n",
        "    \"\"\"\n",
        "    xx, yy = zip(*batch)\n",
        "\n",
        "    if l is None or l < 0:\n",
        "        l = fmax_length(xx)\n",
        "\n",
        "    X = np.zeros((l, len(batch), 31, 31))\n",
        "\n",
        "    for i, x in enumerate(xx):\n",
        "        X[: len(x), i] = x\n",
        "\n",
        "    yy = np.array(yy)\n",
        "\n",
        "    return torch.Tensor(X), torch.Tensor(yy).long()\n",
        "\n",
        "\n",
        "class GlobulesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Load the data from file and provide an interface for the pytorch code.\n",
        "    It is the main interface to the data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        path,\n",
        "        task=\"cleaning\",\n",
        "        maxlength=20,\n",
        "        preprocessing_method=None,\n",
        "        unreliable_downsampling=5000,\n",
        "    ):\n",
        "        self.path = Path(path)\n",
        "        self.preprocessing_method = preprocessing_method\n",
        "        self.maxlength = maxlength\n",
        "        self.task = task\n",
        "        self.unreliable_downsampling = unreliable_downsampling\n",
        "\n",
        "        df = pd.read_csv(self.path / \"dataset.csv\")\n",
        "        self.y = df[\"label\"].tolist()\n",
        "        self.sizes = df[\"size\"].tolist()\n",
        "        self.names = df[\"sequence_name\"].tolist()\n",
        "\n",
        "        self.len = len(self.y)\n",
        "\n",
        "        # Base dataset has 3 labels: 0 \"healthy\" , 1 \"sick/strange\", 2 \"unreliable\"\n",
        "        if self.task == \"cleaning\":\n",
        "            # If cleaning we only have 2 labels: 1 if \"unreliable\", 0 if reliable\n",
        "            self.y = np.array((np.array(self.y) == 2), dtype=int)\n",
        "            if self.unreliable_downsampling:\n",
        "                self.indices = list(np.argwhere(self.y != 1)[:, 0]) + random.sample(\n",
        "                    list(np.argwhere(self.y == 1)[:, 0]), k=unreliable_downsampling\n",
        "                )\n",
        "                self.len = len(self.indices)\n",
        "                print(self.len)\n",
        "\n",
        "        elif self.task == \"classification\":\n",
        "            # If classification we remove (true) unreliables\n",
        "            # Thus we keep only elements which label is 0 or 1 (not 2)\n",
        "            self.indices = [i for i, v in enumerate(self.y) if v != 2]\n",
        "            self.len = len(self.indices)\n",
        "            self.y = np.array(np.array(self.y) == 0, dtype=int)\n",
        "        else:\n",
        "            raise Exception(\"Bad argument\")\n",
        "        # Build a list of list, for each sequence gives a list of the element\n",
        "        # we want to keep\n",
        "        if preprocessing_method == \"uniform\":\n",
        "            self.kept_elements = self.uniform_downsampling()\n",
        "        elif preprocessing_method == \"similarity\":\n",
        "            self.kept_elements = self.similarity_downsampling()\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def class_counts(self):\n",
        "        if self.task == \"classification\":\n",
        "            unique, counts = np.unique(self.y[self.indices], return_counts=True)\n",
        "        else:\n",
        "            if self.task == \"cleaning\" and self.unreliable_downsampling:\n",
        "                unique, counts = np.unique(self.y[self.indices], return_counts=True)\n",
        "            else:\n",
        "                unique, counts = np.unique(self.y, return_counts=True)\n",
        "\n",
        "        return counts\n",
        "\n",
        "    def uniform_downsampling(self):\n",
        "        keptelements = []\n",
        "        for item in tqdm(range(self.len)):\n",
        "            if self.task == \"classification\":\n",
        "                item = self.indices[item]\n",
        "            if self.task == \"cleaning\" and self.unreliable_downsampling:\n",
        "                item = self.indices[item]\n",
        "\n",
        "            if self.sizes[item] > self.maxlength:\n",
        "                keptelements.append(\n",
        "                    random_downsampling(self.sizes[item], self.maxlength)\n",
        "                )\n",
        "            else:\n",
        "                keptelements.append([i for i in range(self.sizes[item])])\n",
        "\n",
        "        return keptelements\n",
        "\n",
        "    def similarity_downsampling(self):\n",
        "        keptelements = []\n",
        "        for item in tqdm(range(self.len)):\n",
        "            if self.task == \"classification\":\n",
        "                item = self.indices[item]\n",
        "            if self.task == \"cleaning\" and self.unreliable_downsampling:\n",
        "                item = self.indices[item]\n",
        "\n",
        "            if self.sizes[item] > self.maxlength:\n",
        "                im = np.array(\n",
        "                    Image.open(self.path / \"img\" / (self.names[item] + \".png\"))\n",
        "                )\n",
        "                im = torch.Tensor(im).view(31, 31, self.sizes[item]).permute(2, 0, 1)\n",
        "                keptelements.append(\n",
        "                    ssim_downsampling(im.unsqueeze(dim=1), self.maxlength)\n",
        "                )\n",
        "            else:\n",
        "                keptelements.append([i for i in range(self.sizes[item])])\n",
        "\n",
        "        return keptelements\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        idx = item\n",
        "        if self.task == \"classification\":\n",
        "            idx = self.indices[item]\n",
        "        if self.task == \"cleaning\" and self.unreliable_downsampling:\n",
        "            idx = self.indices[item]\n",
        "\n",
        "        im = np.array(Image.open(self.path / \"img\" / (self.names[idx] + \".png\")))\n",
        "\n",
        "        if self.preprocessing_method is not None:\n",
        "            im = np.asarray(\n",
        "                [im[:, i * 31 : (i + 1) * 31] for i in self.kept_elements[item]]\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            im = np.asarray(\n",
        "                [im[:, i * 31 : (i + 1) * 31] for i in range(self.sizes[idx])]\n",
        "            )\n",
        "\n",
        "        return im / 255, self.y[idx]\n",
        "\n",
        "    def get_class(self, item):\n",
        "        idx = item\n",
        "        if self.task == \"classification\":\n",
        "            idx = self.indices[item]\n",
        "        if self.task == \"cleaning\" and self.unreliable_downsampling:\n",
        "            idx = self.indices[item]\n",
        "\n",
        "        return self.y[idx]\n",
        "\n",
        "    def get_seq(self, item, preprocessed=False):\n",
        "        idx = item\n",
        "        if self.task == \"classification\":\n",
        "            idx = self.indices[item]\n",
        "        if self.task == \"cleaning\" and self.unreliable_downsampling:\n",
        "            idx = self.indices[item]\n",
        "\n",
        "        im = np.array(Image.open(self.path / \"img\" / (self.names[idx] + \".png\")))\n",
        "\n",
        "        if self.preprocessing_method is not None:\n",
        "            im = np.asarray(\n",
        "                [im[:, i * 31 : (i + 1) * 31] for i in self.kept_elements[item]]\n",
        "            )\n",
        "            l = int(min(self.sizes[idx], self.maxlength))\n",
        "\n",
        "        else:\n",
        "            im = np.asarray(\n",
        "                [im[:, i * 31 : (i + 1) * 31] for i in range(self.sizes[idx])]\n",
        "            )\n",
        "            l = min(self.sizes[idx])\n",
        "\n",
        "        nim = np.zeros((31, 31 * l))\n",
        "        for i in range(l):\n",
        "            nim[:, i * 31 : (i + 1) * 31] = im[i]\n",
        "\n",
        "        return nim / 255, self.y[idx]"
      ],
      "metadata": {
        "id": "nzELLCA2AUk7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmBmvf66efBJ",
        "outputId": "276fe5d9-623a-4c5c-eeee-8a4f59c1145a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptR93JSfeEWk",
        "outputId": "706b056e-211e-47b2-eb6e-8d471d465d0b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone <https://github.com/icannos/redbloodcells_disease_classification/tree/master/Stage_1/test/img>\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgpS66TXhf1h",
        "outputId": "35dd62e6-8ca8-41f8-da38-99436930875c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n",
            "/bin/bash: -c: line 0: `git clone <https://github.com/icannos/redbloodcells_disease_classification/tree/master/Stage_1/test/img>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('/content/drive/MyDrive/redbloodcells_disease_classification\n",
        "/img.db')\n",
        "# Perform operations on the database\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "961OQFm5u1P9",
        "outputId": "9307523c-0510-4487-a076-7dad589abf16"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-b480f1c3b826>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    conn = sqlite3.connect('/content/drive/MyDrive/redbloodcells_disease_classification\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 3)\n"
          ]
        }
      ]
    }
  ]
}